- **[高并发，高可用，高性能的理解](#)**
    - **[高可用](#)**
        - **[架构设计阶段](#)**
            - **[负载均衡](#)**
                - nginx 的负载均衡可以配置心跳健康检查，根据TCP还有HTTP的
                - nginx 的负载均衡可以配置失败重试机制，单位时间内失败次数达到N将踢出服务列表
            - **[限流](#)**
                - 令牌桶限流，桶中固定N个令牌，固定速度往桶中添加令牌，桶满不能添加，桶空不能处理请求
                - 计数器限流，简单粗暴，限制总并发数，连接池，线程池，秒杀并发数都是计数器用法
            - **[熔断降级](#)**
                - 功能降级
                    - 降级的一个重点是要明白哪些服务是可以降级的，哪些是要誓死保护的
                    - 购物车，下单，结算支付，等服务是不能降级的，商品详情的商家信息，相关分类，推荐商品，热销等是可以降级的
                    - 功能降级的一个关注点就是降级之后有没有保底数据，例如推荐系统不可用前端静态展示默认商品
                - 读降级：适合对读一致性要求不高的场景，从读数据库改为读缓存
                - 写降级：秒杀活动中，库存扣减在缓存处理，不直接写数据库
                - 风控降级：抢购秒杀活动中，用风控系统识别机器人等非法请求
                - 超时降级：针对非核心服务，热销榜，推荐商品等暂时不展示不影响
                - 达到失败次数降级
                - 限流降级：请求量太大时，可以引导用户到排队页面
            - **[超时重试](#)**
                - 合理的超时时间与重试次数
                - 数据库客户端超时，缓存读写超时，接口超时，中间件超时
        - **[压测预案阶段](#)**
            - 压测
                - 单机调试，压测接口，压测策略，压测指标，QPS, 响应时间，成功率
                - 压测方式，线下，线上，单接口压测，链路压测
            - 预案
                - IP限流，某些IP访问量过大导致应用负载过高
                - 系统故障，CPU,内存，硬盘，TCP连接数，JVM内存等出现异常的排查和预案
                - 防止爬虫
                - 应用本身资源异常，例如tomcat线程池，连接数，数据库连接池等访问量大时是否能动态监控调整
        - **[线上发布阶段](#)**
            - 增量发布于全量发布
            - 灰度发布，滚动发布
            - 借助容器编排工具实现容器的滚动发布
        - **[出现问题回滚阶段](#)**
            - 事务回滚
            - 代码库回滚
            - 数据库脚本回滚
        - **[线上运行监控报警阶段](#)**
            - 服务器监控，包括CPU, 内存，硬盘，网络，连接数
            - 应用监控，JVM堆内存，异常监控，接口处理监控
    - **[高并发](#)**
        - **[缓存](#)**
            - 缓存永远比数据库，磁盘访问速度快
            - 缓存的基础上还可以用多级缓存，例如CPU的3级缓存L1,L2,L3,一次访问，没有再从内存读取
            - 多级缓存对应到系统中相当于客户端缓存，服务端应用Java缓存，redis缓存，这样一级级访问
            - 有些资源本身就是有过期属性的，例如用户的购物车，验证码
        - **[池化资源](#)**
            - 数据库连接池，tomcat 线程池，tomcat 连接数，Java 线程池，Http连接池
        - **[异步](#)**
            - Future, 线程池提交多个线程，Future.get() 获取最慢的线程返回
        - **[扩容](#)**
            - 单体应用扩容
                - 就是部署多个实例，用 nginx 做负载均衡
                - 缺点就是应用发布，发布有问题其他功能都受影响，要死一起死
            - 功能或者服务拆分
                - 购物车服务，订单服务，库存服务，支付结算服务，商品服务，推荐服务
            - 数据库扩容
                - 分表分库
                    - 分表是为了解决冷热数据的问题，分库是为了解决单数据库实例磁盘IO性能瓶颈的问题
                    - 使用中间价的好处是对应用是隐藏的，侵入性最小
                    - Mycat支持分库分表、读 写分离、跨库弱事务支持，跨库join/分页/排序
                    - 分表可以按照时间维度分，例如按月分表，也可以按照区间维度分，1-2千万，2-3千万
                    - 分库不是越多越好，库越多中间件需要保持的TCP连接数越多
                    - 数据库的连接池不能只维持一个连接的原因是一个连接上存在多个事务是会导致数据不一致的
                - 读写分离
                    - 读写分离是为了解决有些资源本身就是读多写少的场景，例如商品
```
将并发请求量当做一个静态的概念是不恰当的，更准确更形象的比喻是 并发量其实跟
蓄水池的概念差不多，单位时间内请求量的大小是一个动态变化的，相当于池的水量
会根据流进流出的速度变化

增大蓄水池的长宽高，可以增加并发能力，但是于性能处理速度没关系，
增大出水口，相当于提高处理水的速度，相当于提高性能，在系统中对应到单机性能的提高
增加出水口的数量，相当于增加了并行处理的能力，同样可以提高性能，在系统中对应的是分布式环境中
增加服务器的数量

那么提高性能，是可以线性提高并发量的，因为单位时间内，进来的同时也有出去

2 CPU越快，应用的性能一定越好吗？
答：绝对的。只不过CPU和应用性能提升可能不成线性增长的关系，因为应用可能是IO密集型，
应用性能还会受到IO阻塞的影响


CPU越多，应用的性能一定越好吗？
答：大部分情况是的。如果大量锁存在，性能提升可能会大打折扣，因为并行能力会被锁住，
又变成单线程执行了，没有最大的发挥多CPU的作用

服务器越多，并发量一定越大吗？
答：绝对的。服务器增加，CPU和内存资源相应也就越多，并发能力也就会增大，他们之间是线性相关

服务器越多，性能一定越好吗？
答：大部分情况是的。但是单个服务器的效率可能会是下降的，数据一致性问题、同步问题、锁问题，
这些都会导致单个服务器的效率(CPU利用率)下降，所以不是线性相关
```

- 分库分表
```
电商中的订单表设计的时候一般是根据id分表，但是用户查询订单列表时，每次要在多个
表里面查询聚合返回，效率比较低，可以对用户id进行分表，这样每个用户查询的时候不用跨多个表
聚合查询聚合返回
```
