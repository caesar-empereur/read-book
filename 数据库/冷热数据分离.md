## 相同数据库的冷热数据分离
- 哪些场景适合冷热数据分离
  - 数据量庞大
  - 某些数据走到一个状态后，很少进行修改操作
  - 业务方式限制用户只能查询某个时间之前的数据
- 如何使用冷热数据
  - 冷数据用来归档，用户界面限制只能查询操作半年内的数据
  - 正常的业务流程操作的是热数据表，也就是当前的表
  - 业务上限制用户查询跨冷热的数据

## 冷热数据分离实施方案
- 冷热分离之前先限制用户只能操作半年内的数据
- 将2亿大表复制2份出来，一份当冷数据表，一份当热数据表
- 将大表根据数据量确定好冷热数据的时间临界点
- 在冷表，将时间点之后的热数据剔除，成为冷数据表
- 在热表，复制时间点之前的数据出来到另一个表
- 在停止对该原来大表写入的时候，将旧大表与新的热数据表表名互换
- 启动定时器，定时将热数据表的新产生的冷数据迁移到冷数据表里，
  冷数据表也可以根据时间分成多个表
- 冷热数据分离的一个问题是会存在热点问题，当前的并发量基本都集中到一个表中，有读写集中的压力
- 按照日期分表(冷热分离)的时候，一个订单号不知道在哪个表里面，可以把日期信息弄到单号里，查询时解析出日期定位到表

## 一些数据统计
- mysql
  - 15个字段，2亿的数据造出来需要12小时左右
  - 2亿的数据，数据表的磁盘文件大概有 60 GB
  - 复制一个2亿数据的表，大概需要一个小时的时间
  - 冷数据的表删掉最近 1个月 5百万得数据大概要 5 分钟
  - 把5百万的数据从一个大表复制出来插入到一个新的表大概要 3分钟
  - 将一个表的数据复制到另一个表，id数据不会改变



## 冷热数据迁移遇到的问题
- mysql
  - 复制2亿大表的时候报错，出现 The total number of locks exceeds the lock table size
    - 应该是缓冲池的大小或者临时表的大小设置太小了，需要修改
    - 实际测试将  innodb 的缓冲池设置为 5G 大小可以解决这个问题
- mongodb
  - mongodb 的 export 对应 import
    - 导出的数据是json文件，可读性好，容易恢复
    - mongodb 的export出来的json文档2亿数据有80GB
    - mongodb 的json数据impor进去出现分配虚拟内存错误，虚拟内存溢出
    - mongodb 3千万的数据导出需要10分钟,有12GB，导入需要25分钟
  - mongodb 的 dump 对应 restore
    - 导出的文件是 bson文件，可读性差，不容易恢复
    - mongodb 的dump文件出来2亿数据有90GB
    - 2亿的数据90GB导入需要半个小时

```
mongoexport --host localhost:27017 -d build -c member_coupon_document -q {\"receiveTime\":{\"$gte\":\"2023-04-24\b00:00:00\",\"$lte\":\"2023-04-25\b00:00:00\"}} -o D:\document-one.json
```
