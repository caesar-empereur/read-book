## 分库分表按照业务方式有2种
   - 水平切分 : 将一个表的数据分成多个表存储，多个表的结构相同，例如订单表按照月份或者id切分为多个表
   - 水平切分适合查询场景简单，数据量大的业务系统
   - 垂直切分 : 将一个表的字段分成多个表来存储，每个表的数据都是对应字段的全部数据
## 按照物理存储的方式切分
|方案 |解决的问题 |
|----|----|
|**[分表不分库]()**|单表数据量过大，读写性能经过优化后仍存在瓶颈|
|**[分库不分表]()**|单机实例读写性能，连接数瓶颈|
|**[分库分表]()**|数据量过大，单机实例读写性能，连接数瓶颈|


## 分库之后如何实现数据均匀分布
- 数据分布的逻辑跟负载均衡的逻辑类似
    ![2pc](https://github.com/caesar-empereur/read-book/blob/master/photo/mysql/mysql分库分表的数据分布方案.png)

- 如何解决分库后主键ID的唯一性问题？**[分表不分库]()**
    - 单库的id基本是自增的long类型，分库后继续自增的话，就会有id重复的问题
    - id 转换成 uuid 的话，uuid查询存储性能较差，也不利于应用平滑迁移
    - 其实，我们也可以对ID继续拆分，比如对ID进行分段，不同的库表使用不同的ID段
      
      |分库 |第一次分配的id段|第二次分配的id段|
      |----|----|----|
      |分库1|1000-1999|5000-5999|
      |分库2|2000-2999|6000-6999|
      |分库3|3000-3999|7000-7999|
      |分库4|4000-4999|8000-8999|

## **[分片的方案](#)**
- **[根据分片字段的大小范围进行分散](#)**
    - 如果你的id是自增的，而且能保证在进行分库分表后也是自增的，那么能进行很好的改造
    - 以id大小水平切分，而且极有可能不用迁移数据
    - 因为新的数据(热点数据)总在一个库里，很可能导致热点过于集中
- **[根据分片字段的取模进行分散](#)**
    - 将id mod n，然后放入数据库中，这样能够使数据分散，不会有热点的问题
    - 在扩容的时候，是否会有数据迁移的问题，一般的扩容，当然是会有数据迁移的
- **[多个字段分片取模一样的设计](#)**
    ```
    正常情况下，订单数据分表都要确定按照哪个字段分表，有订单号，userid，id。
    一般情况下是用 userid 分表，因为可以把同一个用户的数据分到同一个库同一个表。
    但是用订单号查询的时候，因为不知道在哪个库跟表，要么就全库扫描，要么就维护订单号跟 userid的关系
    用订单号先查到 userid, 再用 userid 找到库跟表查询
    ```
    - 就是订单号的后2位与userid的后2位设计成一样的，这样取模的时候结果是一样
    - 订单号的生成规则位 userid 的后面2位再加上订单其他的唯一号码。
    - 这样userid取模会定位到同一个库跟表，订单号取模也是
    - 这样做的一个好处将同一个用户的用户表，订单表分到一个库里，避免了跨多个库造成分布式事务
- **[有非分片字段的查询怎么处理](#)**
    - 正常情况是订单号或者userid分片，但是遇上商家后台的没有带用户属性的查询，怎么办
    - 后台的查询可能是多条件的复杂查询
    - 这种情况下可以考虑用 离线数据仓库或者 ES
    - 订单数据落库后通过MQ消息或者 binlog 日志同步到数仓或者 ES
    - ES 本身是支持复杂查询的
## **[扩容的方案](#)**
- 根据表的数据增加库的数量(表数量不变，库数量成倍增加)
    - 核心就是表数量不变，通过增加库的数量，把表逐步分散到多个库中
    - 原来的一个库存多个表，扩容后可能一个库只存一个表
    - 库的扩容方式是每次增加一倍，分库的取模方式 用(id%4)/2，因为 对4，对2取模结果是一样的
- 每次成倍的增加库与表的数量
  
## **[历史数据迁移](#)**
- 将旧的单库数据迁移到新的分库
  - 停机迁移数据的发布
      - 关闭业务访问
      - DTS上面新增一个全量的数据复制任务，把单库的数据复制到新的分库中(千万级数据应该10分左右)
      - 切换 TDDL 配置（单库->分库），并重启应用，检查是否生效
      - 开放业务访问入口，提供服务
  - 不停机发布
      - 通过DTS复制某个时间0点前的数据，从单库复制到分库
      - 3点时修改应用数据库连接从单库切换到分库, 修改配置到重启应该几分钟
      - 3点05分开始应用连接的就是新的分库了，这时候老的单库已经停止写入了
      - 再通过DTS增量复制老的单库中今天凌晨之后产生的数据
      - 持续观察一段时间，如果没问题，老的单库就可以下线了
        ![tps-qps](https://github.com/caesar-empereur/read-book/blob/master/photo/mysql/单库到分库的迁移.png)

- 在原分库分表的基础上进行扩容
    - 扩容的数据迁移是跟分片方案息息相关的，有一种方式可以实现部分扩容不用迁移数据
    ```
    有2个库的时候，分库的取模方式 用(id%4)/2，扩容的时候不是增加一个库，而是增加2个库。
    这样 重新取模的时候用 id%4，因为 对4，对2取模结果是一样的，因此历史数据不用迁移。
    但是这个方案只能维持到一个分库范围内，大于4的时候就要迁移历史数据了
    ```
    - 历史数据迁移比较安全的方式是整张表的迁移
- **[从一个小的分库分表迁移到一个大的分库分表](#)**
    - 不停机发布
        - 上线一个新的应用，所有修改插入的操作写2份，一个旧的分库，一个新的分库，查询只查旧库
        - 插入操作对新旧来说都是一样的，删除，修改对新的库无效，因为新库没有数据，但是不会影响一致性，只要能改到旧库
        - 然后启动一个定时任务，分批将旧的库的数据按照新的分库规则迁移到新的库
        - 校验新的库迁完的数据的完整性，一致性
        - 上线新的应用，读写都走到新的库，旧的应用停掉
## **[非分片字段的多维度查询问题](#)**
- **[CQRS（修改与查询分离模式）](#)**
    - 传统的关系型数据库的CURD都是放在一个库里面的
    - 这种CQRS的模式是指数据的修改与查询用2套数据库系统来处理，修改与查询隔离了
    - 这种模式的一个主要原因是海量数据存储，查询条件复杂，传统数据库无法处理
    - 隔离的2套数据库系统可能是异构的，方便各自扩展，数据的修改需要同步到查询的库里面
- 异构数据冗余
    - 买家端的数据按照 userid 分库分表，运营端的再维护一份 es 的数据，双写
- 维护全局二级索引表
    - 其实就是 mysql 非聚集索引的外部实现，仅仅是多一次查询而已，性能不会有太大影响
    - 维护多个维度的字段到分区键的映射关系，多维度查询时先查映射表，再查分库表
    - 例如分区键是 userid，要通过商品code查询时，先查商品code-->userid 映射表
    - 维护的这个操作可以不用放在业务操作的事务里面，通过异步的 mq 消息推送消费
    
## **[分布式的id生成](#)**
```
max      step
10000    10000
```
- 可以参考数据库号码段的方式
- 每次从数据库更新最大值，update max = max + step,并且把当前最大值 max，step 作为号码段放到内存中
- 放到内存中的号码段是 10000-20000
- 用完的时候再去更新取出


## mysql 高可用架构
- mysql MHA 是一个可以实现mysql 主从复制，故障转移的高可用的第三方的工具
- MHA 中包含一个 manager 和 manager 管理下的多个主从复制的集群
- 集群包含主从复制中常见的 master, slave 结构，slave复制master的日志，读写分离
- 一个 manager 可以管理多个主从复制的集群
- MHA 管理的集群中，最少必须有3个节点，一个master, 2个slave
- master 提交事务时，执行的是半同步的复制，只要有超过一台slave应答事务即可提高成功
- master 发生故障时，manager 可以在30秒内从有最新日志的slave中找出一个成为新的master
- 接着将其他的slave复制新的master的差异的日志
