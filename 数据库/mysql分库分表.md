## 分库分表按照业务方式有2种
   * 水平切分 : 将一个表的数据分成多个表存储，多个表的结构相同，例如订单表按照月份或者id切分为多个表
   * 垂直切分 : 将一个表的字段分成多个表来存储，每个表的数据都是对应字段的全部数据
## 按照物理存储的方式切分
|方案 |解决的问题 |
|----|----|
|**[分表不分库]()**|单表数据量过大，读写性能经过优化后仍存在瓶颈|
|**[分库不分表]()**|单机实例读写性能，连接数瓶颈|
|**[分库分表]()**|数据量过大，单机实例读写性能，连接数瓶颈|


## 分库分表的实际方案(需要解决的问题)
- 分库之后如何实现数据均匀分布
  - 数据分布的逻辑跟负载均衡的逻辑类似
    ![2pc](https://github.com/caesar-empereur/read-book/blob/master/photo/mysql/mysql分库分表的数据分布方案.png)

- 如何解决分库后主键ID的唯一性问题？
    - 单库的id基本是自增的long类型，分库后继续自增的话，就会有id重复的问题
    - id 转换成 uuid 的话，uuid查询存储性能较差，也不利于应用平滑迁移
    - 其实，我们也可以对ID继续拆分，比如对ID进行分段，不同的库表使用不同的ID段
      
      |分库 |第一次分配的id段|第二次分配的id段|
      |----|----|----|
      |分库1|1000-1999|5000-5999|
      |分库2|2000-2999|6000-6999|
      |分库3|3000-3999|7000-7999|
      |分库4|4000-4999|8000-8999|

- **[分片的方案](#)**
    - **[根据分片字段的大小范围进行分散](#)**
        - 如果你的id是自增的，而且能保证在进行分库分表后也是自增的，那么能进行很好的改造
        - 以id大小水平切分，而且极有可能不用迁移数据
        - 因为新的数据(热点数据)总在一个库里，很可能导致热点过于集中
    - **[根据分片字段的取模进行分散](#)**
        - 将id mod n，然后放入数据库中，这样能够使数据分散，不会有热点的问题
        - 在扩容的时候，是否会有数据迁移的问题，一般的扩容，当然是会有数据迁移的
    - **[多个字段分片取模一样的设计](#)**
        ```
        正常情况下，订单数据分表都要确定按照哪个字段分表，有订单号，userid，id。
        一般情况下是用 userid 分表，因为可以把同一个用户的数据分到同一个库同一个表。
        但是用订单号查询的时候，因为不知道在哪个库跟表，要么就全库扫描，要么就维护订单号跟 userid的关系
        用订单号先查到 userid, 再用 userid 找到库跟表查询
        ```
        - 就是订单号的后2位与userid的后2位设计成一样的，这样取模的时候结果是一样
        - 订单号的生成规则位 userid 的后面2位再加上订单其他的唯一号码。
        - 这样userid取模会定位到同一个库跟表，订单号取模也是
    - **[有非分片字段的查询怎么处理](#)**
        - 正常情况是订单号或者userid分片，但是遇上商家后台的没有带用户属性的查询，怎么办
        - 后台的查询可能是多条件的复杂查询
        - 这种情况下可以考虑用 离线数据仓库或者 ES
        - 订单数据落库后通过MQ消息或者 binlog 日志同步到数仓或者 ES
        - ES 本身是支持复杂查询的
- **[扩容的方案](#)**
    - 根据表的数据增加库的数量(表数量不变，库数量成倍增加)
        - 核心就是表数量不变，通过增加库的数量，把表逐步分散到多个库中
        - 原来的一个库存多个表，扩容后可能一个库只存一个表
        - 库的扩容方式是每次增加一倍，分库的取模方式 用(id%4)/2，因为 对4，对2取模结果是一样的
    - 每次成倍的增加库与表的数量
- **[历史数据迁移](#)**
    - 原来是分库分表的数量扩容
        - 扩容的数据迁移是跟分片方案息息相关的，有一种方式可以实现部分扩容不用迁移数据
        ```
        有2个库的时候，分库的取模方式 用(id%4)/2，扩容的时候不是增加一个库，而是增加2个库。
        这样 重新取模的时候用 id%4，因为 对4，对2取模结果是一样的，因此历史数据不用迁移。
        但是这个方案只能维持到一个分库范围内，大于4的时候就要迁移历史数据了
        ```
        - 历史数据迁移比较安全的方式是整张表的迁移
    - **[单库单表到多库多表的数据迁移](#)**
        - 停机迁移数据的发布
            - 关闭业务访问
            - DTS上面新增一个全量的数据复制任务，把单库的数据复制到新的分库中(千万级数据应该10分左右)
            - 切换 TDDL 配置（单库->分库），并重启应用，检查是否生效
            - 开放业务访问入口，提供服务
        - 不停机发布
            - 通过DTS复制某个时间0点前的数据，从单库复制到分库
            - 3点时修改应用数据库连接从单库切换到分库, 修改配置到重启应该几分钟
            - 3点05分开始应用连接的就是新的分库了，这时候老的单库已经停止写入了
            - 再通过DTS增量复制老的单库中今天凌晨之后产生的数据
            - 持续观察一段时间，如果没问题，老的单库就可以下线了
- 针对买家端和运营端怎么满足分库分表
    - 数据写2份，一份买家端，分库分表，一份运营端，不分库分表
- **[分布式事务](#)**
    - mysql 本身支持分布式事务，也就是 XA 事务
- **[分布式的id生成](#)**
    ```
    max      step
    10000    10000
    ```
    - 可以参考数据库号码段的方式
    - 每次从数据库更新最大值，update max = max + step,并且把当前最大值 max，step 作为号码段放到内存中
    - 放到内存中的号码段是 10000-20000
    - 用完的时候再去更新取出
    
- 分库分表需要面对的问题与解决方案

    |问题|解决方案|
    |----|--------|
    |原先的事务会变成分布式事务，需要有分布式事务的方案|XA协议或者2阶段提交，成熟产品有seata|
    |'|关联字段冗余|
    |跨节点关联JOIN查询问题|分次查询(第一次查出对应数据，拿出关联字段再查一次)|
    |'|全局表，将所有需要关联查询的表每个节点都存放一份|
    |'|排序字段为分片字段时，通过分片规则就比较容易定位到指定的分片|
    |跨节点排序，分页问题|排序字段为非分片字段时，需要先在分片节点将数据进行排序，然后将分片返回的结果集进行汇总和再次排序|
    |表的主键的全局唯一性问题|表原先的主键为自增的话就要重新设计，需要使用分布式的全局唯一UUID|
    |非分片字段的查询问题|需要在所有的库中执行对应的order by limit操作后返回，然后再同一计算合并结果返回客户端|
    
- mycat 对分表问题的处理
  
    |问题|解决方案|
    |----|--------|
    |非分片字段查询|Mycat无法计算路由，便发送到所有节点上执行,返回结果统一处理|
    |分页排序|Mycat在处理有偏移量的排序分页是另外一套逻辑——改写SQL，limit m,n会变成 limit 0, m+n |
    |跨节点关联JOIN查询问题|单库中的关联查询都查不出，mycat也会查不出来，如果有关联查询，考虑放弃|
    |分布式事务|只保证 prepare 阶段数据一致性的 弱XA事务 |
