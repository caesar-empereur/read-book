## 1 章 mysql 技术体系

- 实例与数据库的区别
  * mysql 数据库是指操作系统文件的集合
  * mysql 实例是系统进程与内存区域的组成
  * 数据库实例才是处理数据的
  
- innodb 存储引擎的特点

    * 支持事务
    * 行锁设计
    * 插入缓冲
    * 二次写
    * 自适应哈希索引
    - innodb 存储引擎架构
    ![innodb](https://github.com/caesar-empereur/read-book/blob/master/photo/mysql/innodb.png)
   
    - mysql 逻辑架构
    ![innodb](https://github.com/caesar-empereur/read-book/blob/master/photo/mysql/mysql逻辑架构.png)


- mysql 的查询缓存是什么
    - mysql 查询的时候会先在查询缓存里面判断是否之前有过相同的查询
    - 是的话会查询缓存的结果直接返回
    - mysql 做更新操作的时候会清空缓存
    - 如果更新操作频繁的话，查询缓存的失效也会非常频繁，因此不建议使用

## 第 4 章 mysql 数据文件

  * **[表都是根据主键顺序组织存放的](#)**，这种存储方式的表称为 **[索引组织表](#)**
  - 索引组织表的意思是 **[mysql的行纪录都是存放在主键索引树的叶子节点上](#)**
  * 表中的数据一定会有主键，没有指定主键，系统也会创建隐式的主键
  * 页 是mysql 磁盘上最小单位的数据存储形式，也叫 块 **[每页的数据大小是16KB](#)**
  * 页存放的行记录也是有硬性定义的，最多允许存放16KB / 2-200行的记录，即7992行记录
  
## 第 5 章 mysql 索引

 - 唯一索引与普通索引的区别
    - 查询区别
        - 唯一索引查询时是根据B树查找，只要有一个满足条件的接返回
        - 普通索引查询时是根据B树查找，一直查找到到出现不满足条件的纪录就结束
        - 查找性能差别不大，因为InnoDB 的数据是按数据页为单位来读写的，查一条纪录是要将页数据加载到内存的
    - 更新区别
        - 更新的时候，如果数据页在内存，就更新内存，否则就会更新到 change buffer,这样就不用将数据从磁盘加载到内存
        - 将 change buffer 中的操作应用到原数据页，得到最新结果的过程称为 merge
        - 唯一索引更新的时候要校验唯一性约束，因此要在内存中判断，没用 change buffer
        - 唯一索引更新的纪录不在内存中，需要从磁盘读取到内存，这时候性能受影响
        - 普通索引更新的纪录不管在不在内存，都是更新到 change buffer
        - 普通索引减少了随机的磁盘io
    - mysql 查询语句强制使用某个索引
        - select * from table use(force) index
 * 树的数据结构
   - 普通二叉树，平衡二叉树，红黑树为什么不适合做文件索引？
   ![普通二叉树索引](https://github.com/caesar-empereur/read-book/blob/master/photo/mysql/普通二叉树索引.png)
   
   * **[如果用平衡二叉树，红黑树来构建索引的话，当数据多的时候，树的深度很大，按照每个树节点只能存一个key,行记录](#)**
   - **[这样在要经过多次磁盘的IO访问，性能相当低，因此这种树结构知识和内存和小文件](#)**


 * B 树分为 **[B-树](#)**，**[B+树](#)**，但都是 **[多路平衡查找](#)** 的树结构
   * **[B-树](#)** 的一个重要特点是 **[非叶子节点也会存储数据](#)**
      - B-树索引的特点是非叶子结点也会存储数据,也就是每个节点的多个key都是有对应的行的主键以及行记录
        ![B-树索引](https://github.com/caesar-empereur/read-book/blob/master/photo/mysql/B-树索引.png)
   * **[B+树](#)** 的一个重要特点是 **[只有叶子节点会存储数据](#)**，所有叶子节点通过链指针形成 **[排序链表](#)**
      - **[为什么B+树设计成只有叶子节点存数据？](#)**
          - 非叶子结点不存储数据，那么就会存储更多的键值，相应的树的阶数（节点的子节点树）就会更大，树就会更矮更胖
          - 如此一来我们查找数据进行磁盘的 IO 次数又会再次减少，数据查询的效率也会更快
      - 索引数据的插入删除需要对索引页文件的B树结构进行插入删除，分裂
      - 索引页文件的大小是16KB,按照每页(每个树结点)能存储1000个key和1000个指针
      - 只需要B树的深度为3，就可以通过索引文件维护 1000*1000*1000=10亿行记录
      - 只需要2到4次磁盘IO就能定位到某一行，而且B树根结点是常驻内存的
    ![innodb](https://github.com/caesar-empereur/read-book/blob/master/photo/mysql/B+树索引新.png)
 * 聚集索引 与 非聚集索引
  
  |索引类型 | 定义 |叶子节点存储的东西 | 定位方式 |
  |-----------|-------------------|------------|------|
  |聚集索引| 按照每张表的主键构造一棵B+树 | 行记录(数据) | 通过搜寻聚集索引B树找到行记录 |
  |非聚集索引| 按照每张表的主键外的字段构造一棵B+树| 聚集索引的键 | 通过非聚集索引B树找到 聚集索引的键，再通过聚集索引树找到行记录 |
  
  * 非聚集索引先通过找到聚集索引的键再查到到对应行记录的过程，出现了多次IO操作，这个叫 **[回表查询](#)**
  * 但不是所有非聚集索引都要经过回表查询，**[覆盖索引不需要回表](#)**
   
 * **[覆盖索引](#)**
   * 从非聚集索引中就可以得到查询的记录，而不需要查询聚集索引中的记录
       * 假设一个表的字段(id, name, sex, age), id是主键，name有索引
       * 当 select id, name from 时是覆盖索引，因为非聚集索引的叶子结点是主键(id)
       * 因此在 name 构建辅助索引的B树上直接找到了id，name 也知道了，不用再去访问聚集索引
       * 当 select id, name, sex from 时不是覆盖索引，因为 sex 信息只在聚集索引的树上，需要回表查询
   * 使用覆盖索引的一个好处是 **[辅助索引不包含整行记录的所有信息](#)**，故其大小要远小于聚集索引，因此可以减少大量的IO操作
 
 * 哈希索引
   * 哈希表的数据存储是没有任何顺序的，因此哈希索引只有在等值查询才能发挥作用，范围查询与排序是失效的，联合所以最左匹配也不行
   
 * **[mysql 索引失效的情况](#)**
   * like 以%开头，索引无效；当like前缀没有%，后缀有%时，索引有效
   * or语句前后没有同时使用索引。当or左右查询字段只有一个是索引，该索引失效
   * 组合索引，不是使用第一列索引，索引失效
   * 不等于操作符是永远不会用到索引的，因此对它的处理只会产生全表扫描，优化方法： key<>0 改为 key>0 or key<0
   * B-tree索引 is null不会走,is not null会走,位图索引 is null,is not null 都会走
 
## mysql 锁
    
  * 死锁
  
      |定义|解决方案|
      |-----------|-------------------|
      |2个事务因争夺资源造成互相等待，没有外力作用无法推进下去|超时机制，超过等待时间就释放，另一个事务也能进行|
      
  * 死锁示例
  
      |时间|事务A|事务B|
      |-----------|-------------------|---------|
      |t1 |select * from table_a where a=1 for update|'|
      |t2 ||select * from table_a where a=2 for update|
      |t3 |select * from table_a where a=2 for update|'|
      |t4 |'|select * from table_a where a=1 for update|
      |t5 |'|error, deadlock|
   * **[死锁排查](#)**
      * 查询是否锁表：show OPEN TABLES where In_use > 0;
      * 查看正在锁的事务：SELECT * FROM INFORMATION_SCHEMA.INNODB_LOCKS; 
      * 查看锁阻塞线程信息：show processlist， 查询到相对应的查询线程id，然后 kill id
      * 查看当前存储引擎的状态： show engine innodb status，包括事务，锁
   - mysql 开启死锁检测
      - innodb_deadlock_detect 设置为ON，发现死锁后，主动回滚死锁链条中的某一个事务
      - 但是死锁检测需要消耗大量的CPU操作，因为每个进来的线程都要检测自己的加入导致当前死锁
   - **[怎么避免行锁对性能的影响](#)**
      - **[innodb行级锁是通过锁索引记录实现的，如果更新的列没建索引是根据主键索引逐行扫描 逐行加锁，释放锁](#)**
      - **[多个事务更新同一行，不会死锁，只是锁等待，但是更新多行，就会造成循环等待，就是死锁](#)**
      - 如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放
      - 行纪录的锁，可以从设计上将一行纪录变成多行纪录来减少死锁冲突
      - 例如一个商家的余额纪录，分成多行纪录，算总余额的时候加起来，增加余额的时候随机加一行纪录
      - **[按照顺序加锁来避免死锁。比如都是按照先拿t1,再拿t2](#)**
      
   - **[锁](#)**
      - **[锁是用来处理多个并发读写的请求](#)**，如果并发只有读，是不用锁的
      - 并发的读是共享锁，是可以并发执行的
      - 并发的请求有写的时候，就是排他锁，一个锁释放了另一个才能执行
      
   - **[间隙锁与幻读](#)**
      - 幻读
         - 幻读出现的条件
          ```
          可重复读的情况下，普通的查询是快照读，是不会看到别人插入的新纪录的，因此幻读是在 当前读的条件下，也就是 select for update
          通过修改数据达到满足 select for update 条件返回导致多出来的数据不能称为幻读，幻读是针对插入的
          
          幻读的一个原因是行锁只能锁住行纪录，行记录之间的间隙无法控制，被插入新纪录之后范围查询，后面查出来
          会出现多出来的纪录，类似幻觉
          ```
      - 间隙锁
          - 间隙就是索引树中行记录之间的空隙，行锁是存在索引上的，只能锁住行记录，间隙锁才能锁住新纪录的插入
          - 间隙锁 和行锁组成了next key lock，目的是为了防止阻止新纪录的插入导致的幻读，间隙锁是在重复读的隔离级别才有的
          - 非唯一索引的辅助索引在间隙锁里面，是会出现多个间隙锁的，需要把满足条件的行记录组成的间隙都锁住
          - **[update 的条件没索引就是锁住主键索引上满足条件的行和间隙，锁内容太多，容易造成死锁](#)**
          - 查询的间隙锁与一个事务中的多次查询的关系
            - **[间隙锁会导致其他事务的更新或者插入造成阻塞，事务中的多次查询只是不同事务的数据可见性](#)**
          ```
          理解间隙的概念必须要知道B树索引的叶子节点是排序的，针对一行纪录的间隙是指这一行纪录的
          左右相邻的值组成的一个开区间，加上右边等值的行纪录锁就形成了闭区间，间隙是左开右闭的
          
          lock in share mode，执行 for update 时，系统会认为你接下来要更新数据，因此会顺便
          给主键索引上满足条件的行加上行锁
          
          间隙锁加锁的条件，只有在查询加上 for update 或者 lock in share mode
          select * from table where c = 7 for update;
          表里面没有 c = 7 的纪录，因此会锁住已有的纪录的 (5, 10) 之间的间隙
          
           查询过程中访问到的对象才会加锁，而加锁的基本单位是next-key lock（前开后闭）
           
           等值查询上MySQL的优化：索引上的等值查询，如果是唯一索引，next-key lock会退化为行锁，
           如果不是唯一索引，需要访问到第一个不满足条件的值，此时next-key lock会退化为间隙锁
           
           范围查询：无论是否是唯一索引，范围查询都需要访问到不满足条件的第一个值为止
           跟间隙锁存在冲突关系的，是“往这个间隙中插入一个记录”这个操作。间隙锁之间都不存在冲突关系
          ```
## 第 7 章

* 事务的特性
 
  |特性|描述|
  |-----------|-------------------|
  |原子性|一个操作，要么成功，要么失败,没有中间状态|
  |一致性|从一个状态到另一个状态的一致性|
  |隔离性|事务与事务之间的操作不能互相影响|
  |持久性|事务一旦提交就应该永久保存|

* 事务的分类

    |事务|描述|
    |-----------|-------------------|
    |扁平事务|事务由多个操作组成，一旦回滚必须回滚所有操作，代价很大|
    |带有保存点的扁平事务|回滚时可以只回滚最新的保存点，但是系统奔溃，所有保存点都会消失|
    |链式事务|提交事务操作和开始下一个事务操作将合并为一个原子操作|
    |嵌套事务|父事务包含子事务，子事务的提交必须在父事务提交才能生效,一个子事务回滚，其他子事务也会回滚|
    |分布式事务|事务的多个操作分布在多个服务器上|
    
- 事务的实现

    - 事务的开始并不是在 begin tran, 而是在事务中的第一个查询，如果要马上启动事务，begin tran with consistent snapshot
    - 事务的 **[原子性](#)** 是通过 **[undo log](#)** 来实现的
    - 事务的 **[持久性](#)** 是通过 **[redo log](#)** 来实现的
        - **[原子性和一致性是为了保障数据可靠性，宕机恢复以及回滚](#)**
        - **[隔离性是为了处理多个并发读写的请求](#)**
    - **[事务的隔离性是通过 (读写锁+MVCC)来实现的](#)**
    - **[事务的一致性是通过原子性，持久性，隔离性来实现的](#)**
  
    - **[日志文件的事务实现](#)**
    
       |'|redo|undo|
       |-----------|-------------------|-------|
       |定义|redolog称为重做日志，用来保证事务的 **[持久性](#)**|undo log用来保证事务的 **[原子性](#)**|
       |日志类型|redo通常是物理日志，记录的是页的物理修改操作(不是操作的语句)|undo是逻辑日志，根据每行记录进行记录|
       |执行过程|redo 每次事务提交时要将所有操作写入redo buffer，再刷新到磁盘，然后被删除覆盖|undo是需要回滚时执行，回滚不是将执行语句撤销，而是执行与语句相反的语句，insert回滚时会执行delete|
   - binlog 与 undolog 的区别
       - **[redolog是innodb才有的，binlog是mysql server 层都有的](#)**
       - **[redolog 大小是固定的，循环写入，刷新到磁盘后会清除，无法用于回滚操作](#)**
       - redo log主要节省的是随机写磁盘的IO消耗(转成顺序写)
       - redolog 可以用于(只写到内存没写到磁盘)的奔溃后数据恢复，就是 crash safe, binlog 是没有 crash safe 能力的
       - **[只要redolog和binlog日志文件成功写入，mysql就能保证数据不丢，就是保证持久化](#)**
       - binlog 不是直接写入到磁盘的，而是写入到binlog cache, 事务提交的时候再刷新到磁盘上的
       - 每个线程是单独的binlog cache, 但是binlog文件是共享的
       - binlog 是mysql写入的日志文件，有 statement, row, mixed 等几种格式
           - statement 格式纪录的是语句的原文，体积小，但是可能导致主从数据不一致
           - row 格式纪录的是一行纪录的前后2条数据
           - row 格式不会导致主从数据不一致，但是文件体积很大，删除10万行数据就要纪录10万行纪录
           - 因此主从生产库上，执行删除的时候不要超过1000行，多的要循环删除，防止大事务
   - redo log 和 binlog 是怎么关联起来的?
       - redo log 和 binlog 有一个共同的数据字段，叫 XID。崩溃恢复的时候，会按顺序扫描 redo log：
       - 如果碰到既有 prepare、又有 commit 的 redo log，就直接提交；
       - 如果碰到只有 parepare、而没有 commit 的 redo log，就拿着 XID 去 binlog 找对应的事务
   - MySQL 怎么知道 binlog 是完整的?
       - 一个事务的 binlog 是有完整格式的：
       - statement 格式的 binlog，最后会有 COMMIT
       - row 格式的 binlog，最后会有一个 XID event
       
   - **[MVCC](#)**
       - 多版本并发控制是为了应对高并发事务, MVCC 比单纯的加行锁更有效, 开销更小
       - 每行数据都存在一个版本，每次数据更新时都更新该版本，每个版本都有row trx_id, 指向每一条 undolog 日志
       - 因此每行纪录的多个版本不是实际物理存在的多个版本，而是通过 row trx_id 指向的undolog 计算出来的
       - 每行数据都有隐藏的2个列，创建版本号，删除版本号，每开启一个事务，系统版本号加一
       - 修改时Copy出当前版本随意修改，各个事务之间无干扰
       - 保存时比较版本号，如果成功提交，则覆盖原记录
       - **[MVCC](#)** 只有在 **[读提交](#)** 和 **[重复读](#)** 隔离级别下起作用
       - **[MVCC本质是对已经存在的数据做版本标记，插入数据是没法处理的](#)**
       
   - **[隔离级别与脏读幻读的关系](#)**
                                     
       |读的种类|描述|解决办法|原理|办法描述
       |-----------|-------------------|--------|-----|-------------|
       |**[脏读(读未提交)](#)** |**[事务A可读到事务B未提交的数据](#)**|**[读提交](#)**|**[MVCC](#)**|事务A必须等事务B提交后才能取|
       |**[不可重复读(虚读)](#)**|**[事务A可读到事务B提交的更新数据，造成前后2次查询出的同一批数据不一样](#)**|**[重复读](#)**|**[MVCC](#)**|事务A 读提交后，事务B提高更新操作不可见|
       |**[幻读](#)**|**[事务A可读到事务B提交的插入数据，造成第二次查询出现多出来的数据，类似幻觉](#)**|**[序列化(或者重复读级别下的间隙锁+行锁=next key lock)](#)**|**[锁](#)**|最高级别，事务按照串行执行，安全性最高|
       
       - 序列化的隔离级别跟可重复读类似，只是查询语句都加了 LOCK IN SHARE MODE
   
   - **[读提交与重复读的原理](#)**
       - **[一致性视图 (决定一个事务能读到什么数据)](#)**
           ```
           1. 未提交事务生成的记录版本，不可见。
           2. 视图生成前，已提交事务的记录版本对当前视图可见。
           3. 视图生成后，新事务产生的记录版本对当前视图不可见。
           4. 自身事务更新永远可见。
           ```
           - **[重复读](#)** 的读事务开启之后，一致性视图会在第一个查询语句的时候生成，其他事务提交后的版本 **[对当前视图不可见](#)**
           - **[读提交](#)** 的读事务开启之后，**[每次都生成新的一致性视图](#)**，其他事务提交的版本 **[对当前视图总是可见](#)**
       - **[MVCC](#)**
       ```
       mysql 每一行记录，除了真实数据之外，还会存在三个隐藏字段，用来记录额外信息。
       • DB_TRX_ID: 事务 id。
       • DB_ROLL_PTR: 回滚指针，指向 undolog。
       • ROW_ID：行 id, 与此次无关。
       
       MySQL InnoDB 里面每个事务都会有一个唯一事务 ID，它在事务开始的时候会跟 InnoDB 的事务系统申请的，
       并且严格按照顺序递增的。每次事务更新数据时，将会生成一个新的数据版本，然后会把当前的事务 id 赋值
       给当前记录的 DB_TRX_ID。并且数据更新记录（1,1000---->1,900）将会记录在 undo log（回滚日志）中，
       然后使用当前记录的 DB_ROLL_PTR 指向 und olog。
       这样 MySQL 就可以通过 DB_ROLL_PTR 找到 undolog 推导出之前版本记录内容
       ```
       ![mysql-mvcc](https://github.com/caesar-empereur/read-book/blob/master/photo/mysql/mysql-mvcc.png)
       - **[当前读于快照读](#)**
           - 快照读相当于 select 操作会去查找 undo log 中的版本链的上一个记录
           - **[当前读会读取一行记录当前的最新的已经提交的事务对应的数据](#)**
           ![mysql当前于快照读](https://github.com/caesar-empereur/read-book/blob/master/photo/mysql/mysql当前与快照读.png)

- purge

  * delete和 update 并不是直接删除数据，而是将数据打上删除标记，索引页构造的B树对应的键也是删除标记
  * puege 操作用于将 delete update 操作真正删除
  * 这么设计的原因是2为了实现mysql的mvcc，一个事务对某行提交修改时，不会真正删除，其他事务对改行也存在改动，因此需要保存改行的前一个状态，是否真正删除需要puege判断是否有另一个事务对改行操作

- 控制事务的语句
  * mysql 默认情况下，事务是自动提交的
  * START TRANSACTION | BEGIN：显式地开启一个事务
  * ROLLBACK 回滚事务
  * SET TRANSACTION 设置事务的隔离级别
  * 隐式提交事务的 语句 alter table, create index, drop table, drop index
  
## mysql 分布式事务

>>**[mysql本身是支持分布式事务的，是由 XA 事务实现的](#)**

  * XA事务由一个或多个资源管理器、一个事务管理器以及一个应用程序组成
  
    * 资源管理器：提供访问事务资源的方法。通常一个数据库就是一个资源管理器
    * 事务管理器：协调参与全局事务中的各个事务。需要和参与全局事务的所有资源管理器进行通信
    * 应用程序：定义事务的边界，指定全局事务中的操作
    
  * mysql XA 事务的操作流程
    * XA START|BEGIN xid
    * XA END xid
    * XA PREPARE xid
    * XA COMMIT xid
    * XA ROLLBACK xid
    * XA RECOVER xid
    
>> show variable like innodb_support_xa  可以查看数据库是否支持 分布式XA事务

  * JAVA XA事务的实例
    * javax.sql包下面有分别的对应XA事务的类，XAConnection, XADatasSource, XAResource
    * 分别建立2个数据库的连接，获取到对应的2个，XAConnection, XAResource, Statement
    * 分别执行2个 XAResource.start()-->Statement.execute()-->XAResource.end()
    * 分别执行 XAResource.prepare()
    * 分别判断 2个prepate的执行结果,都是OK的话，就都执行 XAResource.commit(), 否则 XAResource.rollback()
    
  * 不好的事务提交习惯
    * 循环中提交事务
    * 自动提交事务
    * 长事务(执行时间很长的事务)
    
## mysql 读写分离，主从复制
- 读写分离是指写数据写到主库，读数据读从库，需要做到主从之间的数据同步复制
- 读写分离的方案
    - 客户端直连实现
        - 少了一层 proxy 转发，所以查询性能稍微好一点儿，并且整体架构简单，排查问题更方便
        - 出现主备切换、库迁移等操作的时候，客户端都会感知到，并且需要调整数据库连接信息
    - 代理方式实现
        - 对客户端比较友好。客户端不需要关注后端细节，连接维护、后端信息维护等工作，都是由 proxy 完成
        - proxy 也需要有高可用架构。因此，带 proxy 架构的整体就相对比较复杂
        - proxy 的中间件会承接很多数据库连接，连接数有限制，出问题影响大
- 主从复制原理
    - 主库写入数据到 binlog, 启动 io 线程将binlog传输到从库
    - 从库接手到binlog, 启动一个线程将 binlog 内容执行到从库
    - 主从复制肯定会有延迟，并且有可能导致生产事故
- **[主从复制延迟的解决办法](#)**
    - **[打开并行复制](#)**，是指从库开启多个线程读取 binlog 的日志，加快复制的速度
    - 并行复制的策略有几种，可以按表分发，也可以按行分发，就是将不同事务的sql分给多个sql线程执行
- 主从复制适用的场景
    - 数据库访问量大，读多写少，时效性要求没那么高的场景
    - 复制延迟导致的事故大部分是因为代码里面插入后又查询更新，这种建议直接更新，或者直连主库
    - 读写分离在大部分场景是可适用的，少部分时效性要求高的可以直连主库，根据 2 8原则
    
## mysql tps qps 连接数之间的关系
- 数据库的 QPS = TPS*20, QPS = 连接数*3，这个数据是阿里云的数据库用 sysbentch 工具测试的
- 要达到5W个数据库连接，大概需要 单机16000连接数，4WQPS的 3台
![tps-qps](https://github.com/caesar-empereur/read-book/blob/master/photo/mysql/tps-qps.png)
